{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik2Vdghfa7-d"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/uygarkurt/DCGAN-PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import timeit\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Ef7e8ZLdbKra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"/kaggle/input/celeba-dataset/img_align_celeba\"\n",
        "BATCH_SIZE = 1024\n",
        "IMG_SIZE = 64\n",
        "CHANNELS = 3\n",
        "INPUT_VECTOR_DIM = 100\n",
        "FEATURE_MAP_DIM = 64\n",
        "LR = 2e-4\n",
        "BETA1 = 0.5\n",
        "EPOCHS = 2\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "sIedcgeTbkBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, feature_map_dim, channels):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv_1 = nn.Conv2d(channels, feature_map_dim*2, 4, 2, 1, bias=False) #size [128, 32, 32]\n",
        "        self.conv_2 = nn.Conv2d(feature_map_dim*2, feature_map_dim*4, 4, 2, 1, bias=False) #size [256, 16, 16]\n",
        "        self.conv_3 = nn.Conv2d(feature_map_dim*4, feature_map_dim*8, 4, 2, 1, bias=False) #size [512, 8, 8]\n",
        "        self.conv_4 = nn.Conv2d(feature_map_dim*8, feature_map_dim*16, 4, 2, 1, bias=False) #size [1024, 4, 4]\n",
        "        self.conv_5 = nn.Conv2d(feature_map_dim*16, 1, 4, 1, 0, bias=False) #size [1, 1, 1]\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(feature_map_dim*4)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(feature_map_dim*8)\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(feature_map_dim*16)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = self.conv_1(inp)\n",
        "        x = self.leaky_relu(x)\n",
        "\n",
        "        x = self.conv_2(x)\n",
        "        x = self.batch_norm_1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "\n",
        "        x = self.conv_3(x)\n",
        "        x = self.batch_norm_2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "\n",
        "        x = self.conv_4(x)\n",
        "        x = self.batch_norm_3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "\n",
        "        x = self.conv_5(x)\n",
        "        out = self.sigmoid(x)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "HDJYMnyMblOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator(FEATURE_MAP_DIM, CHANNELS).to(device)\n",
        "\n",
        "x = torch.randn(BATCH_SIZE, CHANNELS, IMG_SIZE, IMG_SIZE).to(device)\n",
        "dis_out = discriminator(x)\n",
        "print(dis_out.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eamNFimfbo9V",
        "outputId": "1521e401-a8f1-43d0-f427-a92cae1210c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_vector_dim, feature_map_dim, channels):\n",
        "        super(Generator, self).__init__()\n",
        "        self.convt_1 = nn.ConvTranspose2d(input_vector_dim, feature_map_dim*16, 4, 1, 0, bias=False) #size [1024, 4, 4]\n",
        "        self.convt_2 = nn.ConvTranspose2d(feature_map_dim*16, feature_map_dim*8, 4, 2, 1, bias=False) #size [512, 8, 8]\n",
        "        self.convt_3 = nn.ConvTranspose2d(feature_map_dim*8, feature_map_dim*4, 4, 2, 1, bias=False) #size [256, 16, 16]\n",
        "        self.convt_4 = nn.ConvTranspose2d(feature_map_dim*4, feature_map_dim*2, 4, 2, 1, bias=False) #size [128, 32, 32]\n",
        "        self.convt_5 = nn.ConvTranspose2d(feature_map_dim*2, channels, 4, 2, 1, bias=False) #size [3, 64, 64]\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(feature_map_dim*16)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(feature_map_dim*8)\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(feature_map_dim*4)\n",
        "        self.batch_norm_4 = nn.BatchNorm2d(feature_map_dim*2)\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = self.convt_1(inp)\n",
        "        x = self.batch_norm_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.convt_2(x)\n",
        "        x = self.batch_norm_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.convt_3(x)\n",
        "        x = self.batch_norm_3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.convt_4(x)\n",
        "        x = self.batch_norm_4(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.convt_5(x)\n",
        "        out = self.tanh(x)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "ddvDKaDyb6XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(INPUT_VECTOR_DIM, FEATURE_MAP_DIM, CHANNELS).to(device)\n",
        "\n",
        "noise = torch.randn(BATCH_SIZE, INPUT_VECTOR_DIM, 1, 1, device=device)\n",
        "gen_out = generator(noise)\n",
        "print(gen_out.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veEUDqvxb-sB",
        "outputId": "56770c82-16ed-4afd-d57c-62c5536ddd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 3, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageFolder(DATA_DIR,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(IMG_SIZE),\n",
        "                               transforms.CenterCrop(IMG_SIZE),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(*((0.5,0.5,0.5),(0.5,0.5,0.5))),\n",
        "                           ]))\n",
        "\n",
        "\n",
        "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "HvMqkZzicEn7",
        "outputId": "8de385d5-4bb3-4ae3-9a17-e6d615f235c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/celeba-dataset/img_align_celeba'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e8a7d474949e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataset = ImageFolder(DATA_DIR,\n\u001b[0m\u001b[1;32m      2\u001b[0m                            transform=transforms.Compose([\n\u001b[1;32m      3\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/celeba-dataset/img_align_celeba'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
        "generator_optimizer = optim.Adam(generator.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "start = timeit.default_timer()\n",
        "for epoch in tqdm(range(EPOCHS), position=0, leave=True):\n",
        "    generator_running_loss = 0\n",
        "    discriminator_running_loss = 0\n",
        "    for idx, data in enumerate(tqdm(dataloader, position=0, leave=True)):\n",
        "        img_data = data[0].to(device) #size [1024, 3, 64, 64]\n",
        "        dummy_labels = data[1] #size [1024]\n",
        "\n",
        "        real_labels = torch.full((dummy_labels.size()), 1., dtype=torch.float).to(device) #size [1024]\n",
        "        fake_labels = torch.full((dummy_labels.size()), 0., dtype=torch.float).to(device)\n",
        "        noise = torch.randn(dummy_labels.size()[0], INPUT_VECTOR_DIM, 1, 1).to(device) #size [1024, 100, 1, 1]\n",
        "\n",
        "\n",
        "        discriminator_real_out = discriminator(img_data).view(-1) #size [1024] .view(-1) to rid unnecessary dimensions\n",
        "        discriminator_real_loss = criterion(discriminator_real_out, real_labels)\n",
        "        discriminator.zero_grad()\n",
        "        discriminator_real_loss.backward()\n",
        "\n",
        "        generator_fake_out = generator(noise) #size [1024, 3, 64, 64]\n",
        "        discriminator_fake_out = discriminator(generator_fake_out.detach()).view(-1) #detach used because we'll calculate it for a second time\n",
        "        discriminator_fake_loss = criterion(discriminator_fake_out, fake_labels)\n",
        "        discriminator_fake_loss.backward()\n",
        "        discriminator_running_loss += discriminator_real_loss.item() + discriminator_fake_loss.item()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        discriminator_fake_out = discriminator(generator_fake_out).view(-1) #calculated it for a second time. So that we won't have to backward graphs a second time\n",
        "        generator_loss = criterion(discriminator_fake_out, real_labels)\n",
        "        generator_running_loss += generator_loss.item()\n",
        "        generator.zero_grad()\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "\n",
        "    discriminator_loss = discriminator_running_loss / 2*(idx + 1)\n",
        "    generator_loss = generator_running_loss / (idx + 1)\n",
        "\n",
        "    print(f\"Discriminator Loss EPOCH {epoch+1}: {discriminator_loss:.4f}\")\n",
        "    print(f\"Generator Loss EPOCH {epoch+1}: {generator_loss:.4f}\")\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Epoch {epoch+1} Generated Images\")\n",
        "    plt.imshow(np.transpose(vutils.make_grid(generator_fake_out[:9], padding=5, normalize=True, nrow=3).cpu(),(1,2,0)))\n",
        "    plt.show()\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "print(f\"Training Time: {stop-start:.2f}s\")"
      ],
      "metadata": {
        "id": "jPaf8-tncFIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "cMQ5equwcL3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_batch = next(iter(dataloader))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:9], padding=5, normalize=True, nrow=3).cpu(),(1,2,0)))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Generated Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(generator_fake_out[:9], padding=5, normalize=True, nrow=3).cpu(),(1,2,0)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LaPgiZ4acMXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise = torch.randn(dummy_labels.size()[0], INPUT_VECTOR_DIM, 1, 1).to(device)\n",
        "generator_fake_out = generator(noise)\n",
        "\n",
        "plt.imshow(generator_fake_out[5].detach().cpu().permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "lPGNRQwycOcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops==0.6.1\n",
        "!pip install matplotlib==3.7.2\n",
        "!pip install numpy==1.23.5\n",
        "!pip install opencv_python==4.8.0.74\n",
        "!pip install PyYAML==6.0\n",
        "!pip install torch==1.11.0\n",
        "!pip install torchvision==0.12.0\n",
        "!pip install tqdm==4.65.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rrLIJFOVP-XP",
        "outputId": "f3480dd7-6cf8-447b-8366-0f0c51e1bdb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops==0.6.1 in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2) (1.16.0)\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b5f8b0aa14e043f68d6cb41c487443d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv_python==4.8.0.74\n",
            "  Using cached opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv_python==4.8.0.74) (1.23.5)\n",
            "Installing collected packages: opencv_python\n",
            "  Attempting uninstall: opencv_python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "Successfully installed opencv_python-4.8.0.74\n",
            "Collecting PyYAML==6.0\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "Successfully installed PyYAML-6.0\n",
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.10.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0\n",
            "Collecting torchvision==0.12.0\n",
            "  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (4.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (2.31.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (1.11.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (2024.2.2)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu121\n",
            "    Uninstalling torchvision-0.16.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu121\n",
            "Successfully installed torchvision-0.12.0\n",
            "Collecting tqdm==4.65.0\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.2\n",
            "    Uninstalling tqdm-4.66.2:\n",
            "      Successfully uninstalled tqdm-4.66.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tqdm-4.65.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class LinearNoiseScheduler:\n",
        "    r\"\"\"\n",
        "    Class for the linear noise scheduler that is used in DDPM.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
        "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
        "\n",
        "    def add_noise(self, original, noise, t):\n",
        "        r\"\"\"\n",
        "        Forward method for diffusion\n",
        "        :param original: Image on which noise is to be applied\n",
        "        :param noise: Random Noise Tensor (from normal dist)\n",
        "        :param t: timestep of the forward process of shape -> (B,)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        original_shape = original.shape\n",
        "        batch_size = original_shape[0]\n",
        "\n",
        "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod[t].reshape(batch_size)\n",
        "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n",
        "\n",
        "        # Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)\n",
        "        for _ in range(len(original_shape)-1):\n",
        "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
        "        for _ in range(len(original_shape)-1):\n",
        "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
        "\n",
        "        # Apply and Return Forward process equation\n",
        "        return (sqrt_alpha_cum_prod.to(original.device) * original\n",
        "                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n",
        "\n",
        "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
        "        r\"\"\"\n",
        "            Use the noise prediction by model to get\n",
        "            xt-1 using xt and the nosie predicted\n",
        "        :param xt: current timestep sample\n",
        "        :param noise_pred: model noise prediction\n",
        "        :param t: current timestep we are at\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        x0 = (xt - (self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred)) / torch.sqrt(self.alpha_cum_prod[t])\n",
        "        x0 = torch.clamp(x0, -1., 1.)\n",
        "\n",
        "        mean = xt - ((self.betas[t])*noise_pred)/(self.sqrt_one_minus_alpha_cum_prod[t])\n",
        "        mean = mean / torch.sqrt(self.alphas[t])\n",
        "\n",
        "        if t == 0:\n",
        "            return mean, mean\n",
        "        else:\n",
        "            variance = (1-self.alpha_cum_prod[t-1]) / (1.0 - self.alpha_cum_prod[t])\n",
        "            variance = variance * self.betas[t]\n",
        "            sigma = variance ** 0.5\n",
        "            z = torch.randn(xt.shape).to(xt.device)\n",
        "\n",
        "            # OR\n",
        "            # variance = self.betas[t]\n",
        "            # sigma = variance ** 0.5\n",
        "            # z = torch.randn(xt.shape).to(xt.device)\n",
        "            return mean + sigma*z, x0"
      ],
      "metadata": {
        "id": "OVl2icSwQ563"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r\"\"\"\n",
        "File to extract csv images from csv files for mnist dataset.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import _csv as csv\n",
        "\n",
        "def extract_images(save_dir, csv_fname):\n",
        "    assert os.path.exists(save_dir), \"Directory {} to save images does not exist\".format(save_dir)\n",
        "    assert os.path.exists(csv_fname), \"Csv file {} does not exist\".format(csv_fname)\n",
        "    with open(csv_fname) as f:\n",
        "        reader = csv.reader(f)\n",
        "        for idx, row in enumerate(reader):\n",
        "            if idx == 0:\n",
        "                continue\n",
        "            im = np.zeros((784))\n",
        "            im[:] = list(map(int, row[1:]))\n",
        "            im = im.reshape((28,28))\n",
        "            if not os.path.exists(os.path.join(save_dir, row[0])):\n",
        "                os.mkdir(os.path.join(save_dir, row[0]))\n",
        "            cv2.imwrite(os.path.join(save_dir, row[0], '{}.png'.format(idx)), im)\n",
        "            if idx % 1000 == 0:\n",
        "                print('Finished creating {} images in {}'.format(idx+1, save_dir))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    extract_images('/content/data/train_images', '../content/sample_data/mnist_train_small.csv')\n",
        "    extract_images('/content/data/test_images', '../content/sample_data/mnist_test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2eNJILfSZpo",
        "outputId": "e6fd9acd-2bb0-4787-f687-8cb9a3c1db06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished creating 1001 images in /content/data/train_images\n",
            "Finished creating 2001 images in /content/data/train_images\n",
            "Finished creating 3001 images in /content/data/train_images\n",
            "Finished creating 4001 images in /content/data/train_images\n",
            "Finished creating 5001 images in /content/data/train_images\n",
            "Finished creating 6001 images in /content/data/train_images\n",
            "Finished creating 7001 images in /content/data/train_images\n",
            "Finished creating 8001 images in /content/data/train_images\n",
            "Finished creating 9001 images in /content/data/train_images\n",
            "Finished creating 10001 images in /content/data/train_images\n",
            "Finished creating 11001 images in /content/data/train_images\n",
            "Finished creating 12001 images in /content/data/train_images\n",
            "Finished creating 13001 images in /content/data/train_images\n",
            "Finished creating 14001 images in /content/data/train_images\n",
            "Finished creating 15001 images in /content/data/train_images\n",
            "Finished creating 16001 images in /content/data/train_images\n",
            "Finished creating 17001 images in /content/data/train_images\n",
            "Finished creating 18001 images in /content/data/train_images\n",
            "Finished creating 19001 images in /content/data/train_images\n",
            "Finished creating 1001 images in /content/data/test_images\n",
            "Finished creating 2001 images in /content/data/test_images\n",
            "Finished creating 3001 images in /content/data/test_images\n",
            "Finished creating 4001 images in /content/data/test_images\n",
            "Finished creating 5001 images in /content/data/test_images\n",
            "Finished creating 6001 images in /content/data/test_images\n",
            "Finished creating 7001 images in /content/data/test_images\n",
            "Finished creating 8001 images in /content/data/test_images\n",
            "Finished creating 9001 images in /content/data/test_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "class MnistDataset(Dataset):\n",
        "    r\"\"\"\n",
        "    Nothing special here. Just a simple dataset class for mnist images.\n",
        "    Created a dataset class rather using torchvision to allow\n",
        "    replacement with any other image dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, split, im_path, im_ext='png'):\n",
        "        r\"\"\"\n",
        "        Init method for initializing the dataset properties\n",
        "        :param split: train/test to locate the image files\n",
        "        :param im_path: root folder of images\n",
        "        :param im_ext: image extension. assumes all\n",
        "        images would be this type.\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        self.im_ext = im_ext\n",
        "        self.images, self.labels = self.load_images('/content/data')\n",
        "\n",
        "    def load_images(self, im_path):\n",
        "        r\"\"\"\n",
        "        Gets all images from the path specified\n",
        "        and stacks them all up\n",
        "        :param im_path:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
        "        ims = []\n",
        "        labels = []\n",
        "        for d_name in tqdm(os.listdir(im_path)):\n",
        "            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n",
        "                ims.append(fname)\n",
        "                labels.append(int(d_name))\n",
        "        print('Found {} images for split {}'.format(len(ims), self.split))\n",
        "        return ims, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im = Image.open(self.images[index])\n",
        "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
        "\n",
        "        # Convert input to -1 to 1 range.\n",
        "        im_tensor = (2 * im_tensor) - 1\n",
        "        return im_tensor"
      ],
      "metadata": {
        "id": "Cc-DLdNNRBfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "\n",
        "r\"\"\"\n",
        "Simple Dataloader for mnist.\n",
        "This assumes the images are already extracted from csv.\n",
        "For extracting images from csv one can use utils/extract_mnist_images\n",
        "\"\"\"\n",
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, split, im_path, im_ext='png'):\n",
        "        self.split = split\n",
        "        self.im_ext = im_ext\n",
        "        self.images, self.labels = self.load_images(im_path)\n",
        "\n",
        "    def load_images(self, im_path):\n",
        "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
        "        ims = []\n",
        "        labels = []\n",
        "        for d_name in tqdm(os.listdir(im_path)):\n",
        "            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n",
        "                ims.append(fname)\n",
        "                labels.append(int(d_name))\n",
        "        print('Found {} images for split {}'.format(len(ims), self.split))\n",
        "        return ims, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im = cv2.imread(self.images[index], 0)\n",
        "        label = self.labels[index]\n",
        "        # Convert to 0 to 255 into -1 to 1\n",
        "        im = 2*(im / 255) - 1\n",
        "        # Convert H,W,C into 1,C,H,W\n",
        "        im_tensor = torch.from_numpy(im)[None,:]\n",
        "        return im_tensor, torch.as_tensor(label)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mnist = MnistDataset('test', '/content/data/test_images')\n",
        "    mnist_loader = DataLoader(mnist, batch_size=16, shuffle=True, num_workers=0)\n",
        "    for im, label in mnist_loader:\n",
        "        print('Image dimension', im.shape)\n",
        "        print('Label dimension: {}'.format(label.shape))\n",
        "        break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_Krw0-4XCKH",
        "outputId": "c78ac7d9-aab2-4eef-917f-2af916366e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 164.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9999 images for split test\n",
            "Image dimension torch.Size([16, 1, 28, 28])\n",
            "Label dimension: torch.Size([16])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def get_time_embedding(time_steps, temb_dim):\n",
        "    r\"\"\"\n",
        "    Convert time steps tensor into an embedding using the\n",
        "    sinusoidal time embedding formula\n",
        "    :param time_steps: 1D tensor of length batch size\n",
        "    :param temb_dim: Dimension of the embedding\n",
        "    :return: BxD embedding representation of B time steps\n",
        "    \"\"\"\n",
        "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
        "\n",
        "    # factor = 10000^(2i/d_model)\n",
        "    factor = 10000 ** ((torch.arange(\n",
        "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
        "    )\n",
        "\n",
        "    # pos / factor\n",
        "    # timesteps B -> B, 1 -> B, temb_dim\n",
        "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
        "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
        "    return t_emb\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Down conv block with attention.\n",
        "    Sequence of following block\n",
        "    1. Resnet block with time embedding\n",
        "    2. Attention block\n",
        "    3. Downsample using 2x2 average pooling\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
        "                 down_sample=True, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.down_sample = down_sample\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
        "                              kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels,\n",
        "                              kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [nn.GroupNorm(8, out_channels)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
        "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            # Resnet block of Unet\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            # Attention block of Unet\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "        out = self.down_sample_conv(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MidBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Mid conv block with attention.\n",
        "    Sequence of following blocks\n",
        "    1. Resnet block with time embedding\n",
        "    2. Attention block\n",
        "    3. Resnet block with time embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                              padding=1),\n",
        "                )\n",
        "                for i in range(num_layers+1)\n",
        "            ]\n",
        "        )\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "            for _ in range(num_layers + 1)\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers+1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [nn.GroupNorm(8, out_channels)\n",
        "                for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                for _ in range(num_layers)]\n",
        "        )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers+1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        out = x\n",
        "\n",
        "        # First resnet block\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first[0](out)\n",
        "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
        "        out = self.resnet_conv_second[0](out)\n",
        "        out = out + self.residual_input_conv[0](resnet_input)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            # Attention Block\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "            # Resnet Block\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i+1](out)\n",
        "            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i+1](out)\n",
        "            out = out + self.residual_input_conv[i+1](resnet_input)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Up conv block with attention.\n",
        "    Sequence of following blocks\n",
        "    1. Upsample\n",
        "    1. Concatenate Down block output\n",
        "    2. Resnet block with time embedding\n",
        "    3. Attention Block\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.up_sample = up_sample\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                              padding=1),\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [\n",
        "                nn.GroupNorm(8, out_channels)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [\n",
        "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
        "                                                 4, 2, 1) \\\n",
        "            if self.up_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, out_down, t_emb):\n",
        "        x = self.up_sample_conv(x)\n",
        "        x = torch.cat([x, out_down], dim=1)\n",
        "\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    r\"\"\"\n",
        "    Unet model comprising\n",
        "    Down blocks, Midblocks and Uplocks\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config):\n",
        "        super().__init__()\n",
        "        im_channels = model_config['im_channels']\n",
        "        self.down_channels = model_config['down_channels']\n",
        "        self.mid_channels = model_config['mid_channels']\n",
        "        self.t_emb_dim = model_config['time_emb_dim']\n",
        "        self.down_sample = model_config['down_sample']\n",
        "        self.num_down_layers = model_config['num_down_layers']\n",
        "        self.num_mid_layers = model_config['num_mid_layers']\n",
        "        self.num_up_layers = model_config['num_up_layers']\n",
        "\n",
        "        assert self.mid_channels[0] == self.down_channels[-1]\n",
        "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
        "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
        "\n",
        "        # Initial projection from sinusoidal time embedding\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
        "        )\n",
        "\n",
        "        self.up_sample = list(reversed(self.down_sample))\n",
        "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        for i in range(len(self.down_channels)-1):\n",
        "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim,\n",
        "                                        down_sample=self.down_sample[i], num_layers=self.num_down_layers))\n",
        "\n",
        "        self.mids = nn.ModuleList([])\n",
        "        for i in range(len(self.mid_channels)-1):\n",
        "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim,\n",
        "                                      num_layers=self.num_mid_layers))\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for i in reversed(range(len(self.down_channels)-1)):\n",
        "            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n",
        "                                    self.t_emb_dim, up_sample=self.down_sample[i], num_layers=self.num_up_layers))\n",
        "\n",
        "        self.norm_out = nn.GroupNorm(8, 16)\n",
        "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
        "        # Shapes assuming midblocks are [C4, C4, C3]\n",
        "        # Shapes assuming downsamples are [True, True, False]\n",
        "        # B x C x H x W\n",
        "        out = self.conv_in(x)\n",
        "        # B x C1 x H x W\n",
        "\n",
        "        # t_emb -> B x t_emb_dim\n",
        "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
        "        t_emb = self.t_proj(t_emb)\n",
        "\n",
        "        down_outs = []\n",
        "\n",
        "        for idx, down in enumerate(self.downs):\n",
        "            down_outs.append(out)\n",
        "            out = down(out, t_emb)\n",
        "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
        "        # out B x C4 x H/4 x W/4\n",
        "\n",
        "        for mid in self.mids:\n",
        "            out = mid(out, t_emb)\n",
        "        # out B x C3 x H/4 x W/4\n",
        "\n",
        "        for up in self.ups:\n",
        "            down_out = down_outs.pop()\n",
        "            out = up(out, down_out, t_emb)\n",
        "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
        "        out = self.norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.conv_out(out)\n",
        "        # out B x C x H x W\n",
        "        return out"
      ],
      "metadata": {
        "id": "WaI_paTaWAKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import yaml\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    # Read the config file #\n",
        "    with open(args.config_path, 'r') as file:\n",
        "        try:\n",
        "            config = yaml.safe_load(file)\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(exc)\n",
        "    print(config)\n",
        "    ########################\n",
        "\n",
        "    diffusion_config = config['diffusion_params']\n",
        "    dataset_config = config['dataset_params']\n",
        "    model_config = config['model_params']\n",
        "    train_config = config['train_params']\n",
        "\n",
        "    # Create the noise scheduler\n",
        "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
        "                                     beta_start=diffusion_config['beta_start'],\n",
        "                                     beta_end=diffusion_config['beta_end'])\n",
        "\n",
        "    # Create the dataset\n",
        "    mnist = MnistDataset('train', im_path=dataset_config['im_path'])\n",
        "    mnist_loader = DataLoader(mnist, batch_size=train_config['batch_size'], shuffle=True, num_workers=4)\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = Unet(model_config).to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Create output directories\n",
        "    if not os.path.exists(train_config['task_name']):\n",
        "        os.mkdir(train_config['task_name'])\n",
        "\n",
        "    # Load checkpoint if found\n",
        "    if os.path.exists(os.path.join(train_config['task_name'],train_config['ckpt_name'])):\n",
        "        print('Loading checkpoint as found one')\n",
        "        model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
        "                                                      train_config['ckpt_name']), map_location=device))\n",
        "    # Specify training parameters\n",
        "    num_epochs = train_config['num_epochs']\n",
        "    optimizer = Adam(model.parameters(), lr=train_config['lr'])\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    # Run training\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        losses = []\n",
        "        for im, labels in tqdm(mnist_loader):\n",
        "            optimizer.zero_grad()\n",
        "            im = im.float().to(device)\n",
        "\n",
        "            # Sample random noise\n",
        "            noise = torch.randn_like(im).to(device)\n",
        "\n",
        "            # Sample timestep\n",
        "            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n",
        "\n",
        "            # Add noise to images according to timestep\n",
        "            noisy_im = scheduler.add_noise(im, noise, t)\n",
        "            noise_pred = model(noisy_im, t)\n",
        "\n",
        "            loss = criterion(noise_pred, noise)\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print('Finished epoch:{} | Loss : {:.4f}'.format(\n",
        "            epoch_idx + 1,\n",
        "            np.mean(losses),\n",
        "        ))\n",
        "        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n",
        "                                                    train_config['ckpt_name']))\n",
        "\n",
        "    print('Done Training ...')\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set the default config path\n",
        "    default_config_path = '/content/default.yaml'\n",
        "\n",
        "    # Remove unwanted arguments from sys.argv\n",
        "    sys.argv = [arg for arg in sys.argv if 'kernel-' not in arg]\n",
        "\n",
        "    # Create the argument parser\n",
        "    parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n",
        "\n",
        "    # Add the config argument with 'nargs='?' to make it optional\n",
        "    parser.add_argument('--config', dest='config_path', nargs='?', type=str)\n",
        "\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # If config_path is not provided, use the default config path\n",
        "    if args.config_path is None:\n",
        "        args.config_path = default_config_path\n",
        "\n",
        "    # Check if the provided config path exists\n",
        "    if not os.path.isfile(args.config_path):\n",
        "        raise FileNotFoundError(f\"Config file not found at {args.config_path}\")\n",
        "\n",
        "    # Call the train function with the updated args\n",
        "    train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "HaZ30LGWWkSi",
        "outputId": "956243fb-f3c8-4240-9c15-63675948bf4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dataset_params': {'im_path': '/content/data/train_images'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'im_channels': 1, 'im_size': 28, 'down_channels': [32, 64, 128, 256], 'mid_channels': [256, 256, 128], 'down_sample': [True, True, False], 'time_emb_dim': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'num_heads': 4}, 'train_params': {'task_name': 'default', 'batch_size': 64, 'num_epochs': 40, 'num_samples': 100, 'num_grid_rows': 10, 'lr': 0.0001, 'ckpt_name': 'ddpm_ckpt.pth'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 176.28it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19999 images for split train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 30/313 [11:31<1:48:39, 23.04s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-1bfd7cf88d12>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# Call the train function with the updated args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-1bfd7cf88d12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Add noise to images according to timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mnoisy_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mnoise_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ce7eb2a1019c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mdown_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdown_outs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0;31m# out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ce7eb2a1019c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, out_down, t_emb)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0min_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_norms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0min_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mout_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0mout_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 v_proj_weight=self.v_proj_weight, average_attn_weights=average_attn_weights)\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1039\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5356\u001b[0m     \u001b[0;31m# (deep breath) calculate attention and out projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5357\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5358\u001b[0;31m     \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_scaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5359\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5360\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   5035\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5036\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5037\u001b[0;31m     \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5038\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5039\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1818\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1819\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import argparse\n",
        "import yaml\n",
        "import os\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def sample(model, scheduler, train_config, model_config, diffusion_config):\n",
        "    r\"\"\"\n",
        "    Sample stepwise by going backward one timestep at a time.\n",
        "    We save the x0 predictions\n",
        "    \"\"\"\n",
        "    xt = torch.randn((train_config['num_samples'],\n",
        "                      model_config['im_channels'],\n",
        "                      model_config['im_size'],\n",
        "                      model_config['im_size'])).to(device)\n",
        "    for i in tqdm(reversed(range(diffusion_config['num_timesteps']))):\n",
        "        # Get prediction of noise\n",
        "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
        "\n",
        "        # Use scheduler to get x0 and xt-1\n",
        "        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
        "\n",
        "        # Save x0\n",
        "        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n",
        "        ims = (ims + 1) / 2\n",
        "        grid = make_grid(ims, nrow=train_config['num_grid_rows'])\n",
        "        img = torchvision.transforms.ToPILImage()(grid)\n",
        "        if not os.path.exists(os.path.join(train_config['task_name'], 'samples')):\n",
        "            os.mkdir(os.path.join(train_config['task_name'], 'samples'))\n",
        "        img.save(os.path.join(train_config['task_name'], 'samples', 'x0_{}.png'.format(i)))\n",
        "        img.close()\n",
        "\n",
        "\n",
        "def infer(args):\n",
        "    # Read the config file #\n",
        "    with open(args.config_path, 'r') as file:\n",
        "        try:\n",
        "            config = yaml.safe_load(file)\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(exc)\n",
        "    print(config)\n",
        "    ########################\n",
        "\n",
        "    diffusion_config = config['diffusion_params']\n",
        "    model_config = config['model_params']\n",
        "    train_config = config['train_params']\n",
        "\n",
        "    # Load model with checkpoint\n",
        "    model = Unet(model_config).to(device)\n",
        "    model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
        "                                                  train_config['ckpt_name']), map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Create the noise scheduler\n",
        "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
        "                                     beta_start=diffusion_config['beta_start'],\n",
        "                                     beta_end=diffusion_config['beta_end'])\n",
        "    with torch.no_grad():\n",
        "        sample(model, scheduler, train_config, model_config, diffusion_config)\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set the default config path\n",
        "    default_config_path = '/content/default.yaml'\n",
        "\n",
        "    # Remove unwanted arguments from sys.argv\n",
        "    sys.argv = [arg for arg in sys.argv if 'kernel-' not in arg]\n",
        "\n",
        "    # Create the argument parser\n",
        "    parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n",
        "\n",
        "    # Add the config argument with 'nargs='?' to make it optional\n",
        "    parser.add_argument('--config', dest='config_path', nargs='?', type=str)\n",
        "\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # If config_path is not provided, use the default config path\n",
        "    if args.config_path is None:\n",
        "        args.config_path = default_config_path\n",
        "\n",
        "    # Check if the provided config path exists\n",
        "    if not os.path.isfile(args.config_path):\n",
        "        raise FileNotFoundError(f\"Config file not found at {args.config_path}\")\n",
        "\n",
        "    # Call the train function with the updated args\n",
        "    infer(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "q-NK0A_RZsAF",
        "outputId": "6a6a3af4-c137-4406-96da-5d5b0913464d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dataset_params': {'im_path': '/content/data/test_images'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'im_channels': 1, 'im_size': 28, 'down_channels': [32, 64, 128, 256], 'mid_channels': [256, 256, 128], 'down_sample': [True, True, False], 'time_emb_dim': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'num_heads': 4}, 'train_params': {'task_name': 'default', 'batch_size': 64, 'num_epochs': 40, 'num_samples': 100, 'num_grid_rows': 10, 'lr': 0.0001, 'ckpt_name': 'ddpm_ckpt.pth'}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'default/ddpm_ckpt.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-908056fc14c6>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Call the train function with the updated args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-908056fc14c6>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Load model with checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n\u001b[0m\u001b[1;32m     56\u001b[0m                                                   train_config['ckpt_name']), map_location=device))\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'default/ddpm_ckpt.pth'"
          ]
        }
      ]
    }
  ]
}